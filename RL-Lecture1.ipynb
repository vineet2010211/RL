{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Deep Mind's David Silver's Course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture 1 - Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reinforcement learning has many aspects and has distinctive application in not only machine learning but fields such as robotics and control theory. Many faces of reinforcement learning:\n",
    "\n",
    "1. Machine Learning\n",
    "2. Reward System\n",
    "3. Operations Research\n",
    "4. Bounded Rationality \n",
    "5. Optimal Control \n",
    "6. Classical/Operant Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  ##### Characterstics of Reinforcement Learning \n",
    "\n",
    "What makes reinforcement learning differen from other machine learning paradigms ? \n",
    "    1. There is no supervisor, only a ${Reward}$ signal.\n",
    "    2. Feedback is delayed, not instantaneous.\n",
    "    3. Time really matters, sequential decision making (non i.i.d data).\n",
    "    4. Agent's action affect the subsequent data it recieves.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline \n",
    "- <font color='grey'>About Reinforcement Learning.</font>\n",
    "- The Reinforcement Learning Problem.\n",
    "- <font color='grey'>Inside An RL agent.</font>\n",
    "- <font color='grey'>Problems with Reinforcement Learning.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Reinforcement Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Rewards\n",
    "\n",
    "- A reward $R_t$ is a scalar feedback signal.\n",
    "- Indicates how the agent is doing at time step t. \n",
    "- Agent's job is to maximize cumulative reward.\n",
    "\n",
    "Reinforcement Learning is based on $\\textbf{Reward hypothesis}$.\n",
    "> All goals can be described by the maximization of expected reward.\n",
    "\n",
    "###### Examples of Rewards \n",
    "\n",
    "- Fly stunt manoeuvres in a helicopter.\n",
    "    - +ve reward for following desired trajectory.\n",
    "    - -ve reward for crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "- Goal: select actions to maximize total futre reward.\n",
    "> That usually means planning ahead.\n",
    "- Actions may have long term consequences.\n",
    "- Reward may be delayed.\n",
    "- It may be better to sacrifice immediate reward to gain more long-term reward.\n",
    "- Examples:\n",
    "    - A financial investment(may take months to mature)\n",
    "    - Refueling a helicopter (might prevent a crash in several hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agent and Environment \n",
    "Interaction between agent and environment\n",
    "Trial and Error Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observations $O_t$\n",
    "Action $A_t$\n",
    "Reward $R_t$\n",
    "\n",
    "- At each time step ${t}$ the $\\textbf{agent}$:\n",
    "    - Executes action $A_t$\n",
    "    - Receives observation $O_t$\n",
    "    - Receives scalar reward $R_t$\n",
    "\n",
    "- At each time step ${t}$, the $\\textbf{environment}$:\n",
    "    - Receives action $A_t$\n",
    "    - Emits observation $O_t$\n",
    "    - Emits scalar reward $R_t$\n",
    "\n",
    "The Reward can always be modelled as a Scalar: This is the RL view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The history is the sequence of observations, rewards and actions $ H_t = A_1,O_1,R_1 .... A_t,O_t,R_t $\n",
    "- i.e all the obsevable variables upto time $t$.\n",
    "- What happens next depends on the history.\n",
    "    -An agent selects actions.\n",
    "    - The environment selects observations/rewards.\n",
    "- $State$ is the information used to determine what happens next.\n",
    "- Formally, state is a function of the history. $S_t = f(H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Environment State   $S_t^e$\n",
    "\n",
    "- It is the environments's private representation.\n",
    "- It is whatever data environment uses to pick the next observation/reward.\n",
    "- The environment state is usually not visible to the agent.\n",
    "- Even if $S_t^e$ is visible, it may not contain the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Agent State $S_t^a$\n",
    "\n",
    "- It is the agents internal representation\n",
    "- Whatever data agent uses to pick the next action. i.e. the information used by reinforcement learning algorithms.\n",
    "- It can be any function history: $S_t^a = f(H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information State or Markov State\n",
    "An information state contains all the information from the history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Definition \n",
    "A state is Markov if and only if \n",
    "   > $P[S_{t+1} | S_t] = P[S_{t+1} | S_1,S_2,S_3,S_4 ....S_t]$\n",
    "\n",
    "######  \"The future is independent of the past given the present\"\n",
    "> $H_{1:t} -> S_t -> H_{t+1:\\infty}$\n",
    "\n",
    "- Once the state is known, the history can be thrown away.\n",
    "- The state is sufficient statistics of the future.\n",
    "- The environment state $S_t^e$ is Markov.\n",
    "- The history $H_t$ is Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- State representation really defines what happens next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fully Observable Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Agent directly observs environment state $O_t = S_t^a = S_t^e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Agent State = Information State = Information state\n",
    "- Formally this is Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Partially Observable Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Parial Observability: agent indirectly observes environment.\n",
    "    - A robot with camera vision isn't told it's absolute location.\n",
    "    - A trading Agent only observs current prices.\n",
    "    - A poker playing agent only observes public cards.\n",
    "\n",
    "- Now agent state $\\neq$ environment state.\n",
    "- Formally this is a partially observable Markov Decision Process (POMDP)\n",
    "- Agent must construct its own state $S_t^a$, e.g.\n",
    "    - Complete History: $S_t^a = H_t$\n",
    "    - Beliefs of environment state: $S_t^a = (P[S_t^e = S^1], .......P[S_t^e=S^n])$\n",
    "    - Recurrent neural network: $S_t^a = \\sigma(S_{t-1} W_s + O_t W_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline \n",
    "- <font color='grey'>About Reinforcement Learning.</font>\n",
    "- <font color='grey'>The Reinforcement Learning Problem. </font>\n",
    "- Inside An RL agent.\n",
    "- <font color='grey'>Problems with Reinforcement Learning.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inside An RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Major Component of an RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An RL agent may include one or more of these components:\n",
    "    - $\\textbf{Policy}$: Agent's behaviour function.\n",
    "    - $\\textbf{Value Function}$: how good is each state and/or action.\n",
    "    - $\\textbf{Model}$: Agent's representation of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Policy \n",
    "- A policy is the agent's behaviour.\n",
    "- It is a map from state to action, e.g.\n",
    "- Deterministic policy: $a = \\Pi(s) $\n",
    "- Stochastic Policy: $\\Pi(a|s) = P[A=a | S=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Value Function\n",
    "- Value function is a prediction of future reward. \n",
    "- Used to evaluate the goodness/badness of states.\n",
    "- And therefore to select between actions, e.g.\n",
    "    > $V_{\\Pi}(s) = E_{\\Pi}[R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2} + ..... | S_t = s]$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A model predicts what the environment will do next.\n",
    "- $\\textbf{Transitions}$: predicts the next state (i.e. dynamics)\n",
    "- $\\textbf{Rewards}$: Predicts the next immediate reward e.g. \n",
    "   - $\\mathbb{P}_{ss'}^a = \\mathbb{P}[S'=s' | S = s, A = a]$.\n",
    "   - $\\mathfrak{R}_s^a = \\mathbb{E}[R|S = s, A =a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorizing RL agents(1)\n",
    "\n",
    "- Value Based\n",
    "    - <font color='grey'>No Policy (implicit)</font>\n",
    "    - Value Function\n",
    "\n",
    "- Policy Based\n",
    "    - Policy\n",
    "    - <font color='grey'>No Value Function</font>\n",
    "- Actor Critic \n",
    "    - Policy\n",
    "    - Value Function \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorising RL Agents(2)\n",
    "- Model Free \n",
    "    - Policy and/or Valur Function \n",
    "    - <font color='grey'> No Model </font>\n",
    "- Model Based \n",
    "    - Policy and/or Value Function \n",
    "    - Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline \n",
    "- <font color='grey'>About Reinforcement Learning.</font>\n",
    "- <font color='grey'>The Reinforcement Learning Problem. </font>\n",
    "- <font color='grey'>Inside An RL agent.</font>\n",
    "- Problems with Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems within Reinforcement Learning\n",
    "Two fundamental problems in sequential decision making\n",
    "\n",
    "- Reinforcement Learning:\n",
    "    - The environment is initially unknown.\n",
    "    - The agent interacts with the environment.\n",
    "    - The agent improves its policy.\n",
    "- Planning: \n",
    "    - A model of the environment is known.\n",
    "    - The agent performs computations with its model (without any external interaction)\n",
    "    - The agent improves its policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploration vs Exploitation (1)\n",
    "- Reinforcement Learning is like trial and error learning\n",
    "- The agent should discover a good policy.\n",
    "- From its experiences of the environment.\n",
    "- Without loosing too much reward along the way.\n",
    "\n",
    "> Exploration finds more information about the environment.\n",
    "\n",
    "> Exploitation exploits known information to maximize reward.\n",
    "\n",
    "\n",
    "- It is usually imporatant to explore as well as exploit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction and Control\n",
    "\n",
    "- Prediction: evaluate the future\n",
    "    - Given a policy.\n",
    "- Control: optimize the future.\n",
    "    - Find the best policy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
