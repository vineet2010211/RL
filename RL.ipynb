{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Mind's David Silver's Course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture 1 - Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reinforcement learning has many aspects and has distinctive application in not only machine learning but fields such as robotics and control theory. Many faces of reinforcement learning:\n",
    "\n",
    "1. Machine Learning\n",
    "2. Reward System\n",
    "3. Operations Research\n",
    "4. Bounded Rationality \n",
    "5. Optimal Control \n",
    "6. Classical/Operant Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  ##### Characterstics of Reinforcement Learning \n",
    "\n",
    "What makes reinforcement learning differen from other machine learning paradigms ? \n",
    "    1. There is no supervisor, only a ${Reward}$ signal.\n",
    "    2. Feedback is delayed, not instantaneous.\n",
    "    3. Time really matters, sequential decision making (non i.i.d data).\n",
    "    4. Agent's action affect the subsequent data it recieves.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reinforcement Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- A reward $R_t$ is a scalar feedback signal.\n",
    "- Indicates how the agent is doing at time step t. \n",
    "- Agent's job is to maximize cumulative reward.\n",
    "\n",
    "Reinforcement Learning is based on $\\textbf{Reward hypothesis}$.\n",
    "> All goals can be described by the maximization of expected reward.\n",
    "\n",
    "###### Examples of Rewards \n",
    "\n",
    "- Fly stunt manoeuvres in a helicopter.\n",
    "    - +ve reward for following desired trajectory.\n",
    "    - -ve reward for crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Decision Making\n",
    "\n",
    "- Goal: select actions to maximize total futre reward.\n",
    "> That usually means planning ahead.\n",
    "- Actions may have long term consequences.\n",
    "- Reward may be delayed.\n",
    "- It may be better to sacrifice immediate reward to gain more long-term reward.\n",
    "- Examples:\n",
    "    - A financial investment(may take months to mature)\n",
    "    - Refueling a helicopter (might prevent a crash in several hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations $O_t$\n",
    "Action $A_t$\n",
    "Reward $R_t$\n",
    "\n",
    "- At each time step ${t}$ the $\\textbf{agent}$:\n",
    "    - Executes action $A_t$\n",
    "    - Receives observation $O_t$\n",
    "    - Receives scalar reward $R_t$\n",
    "\n",
    "- At each time step ${t}$, the $\\textbf{environment}$:\n",
    "    - Receives action $A_t$\n",
    "    - Emits observation $O_t$\n",
    "    - Emits scalar reward $R_t$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The history is the sequence of observations, rewards and actions $ H_t = A_1,O_1,R_1 .... A_t,O_t,R_t $\n",
    "- i.e all the obsevable variables upto time $t$.\n",
    "- What happens next depends on the history.\n",
    "    -An agent selects actions.\n",
    "    - The environment selects observatiobs/rewards.\n",
    "- $State$ is the information used to determine what happens next.\n",
    "- Formally, state id a function of the history. $S_t = f(H_t)$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
